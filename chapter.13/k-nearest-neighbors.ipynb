{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scores=[22.395209580838323, 24.550898203592812, 26.82634730538922, 26.107784431137727, 24.67065868263473]'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'mean accuracy=24.910179640718564'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import seed, randrange\n",
    "from math import sqrt\n",
    "from csv import reader\n",
    "\n",
    "\n",
    "seed(42)\n",
    "\n",
    "\n",
    "def load_csv(filepath):\n",
    "  dataset = []\n",
    "\n",
    "  with open(filepath, \"r\") as file:\n",
    "    csv_reader = reader(file)\n",
    "    for row in csv_reader:\n",
    "      if row:\n",
    "        dataset.append(row)\n",
    "  \n",
    "  return dataset\n",
    "\n",
    "\n",
    "def str_col_to_float(dataset, col):\n",
    "  for row in dataset:\n",
    "    row[col] = float(row[col].strip())\n",
    "\n",
    "\n",
    "def str_col_to_int(dataset, col):\n",
    "  class_vals = set(list(map(lambda row: row[col], dataset)))\n",
    "  class_lookup = {}\n",
    "\n",
    "  for i, value in enumerate(class_vals):\n",
    "    class_lookup[value] = i\n",
    "  for row in dataset:\n",
    "    row[col] = class_lookup[row[col]]\n",
    "  \n",
    "  return class_lookup\n",
    "\n",
    "\n",
    "def dataset_minmax(dataset):\n",
    "  minmax = []\n",
    "\n",
    "  for i in range(len(dataset)):\n",
    "    col_vals = list(map(lambda row: row[i], dataset))\n",
    "    minmax.append([min(col_vals), max(col_vals)])\n",
    "  \n",
    "  return minmax\n",
    "\n",
    "\n",
    "def normalize_dataset(dataset):\n",
    "  minmax = dataset_minmax(dataset=dataset)\n",
    "  for row in dataset:\n",
    "    for i in range(len(row)):\n",
    "      row[i] = (row[i] - minmax[i][0]) / (minmax[i][1] - minmax[i][0])\n",
    "\n",
    "\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "  dataset_folds = []\n",
    "  fold_size = int(len(dataset) / n_folds)\n",
    "  dataset_ = dataset.copy()\n",
    "\n",
    "  for _ in range(n_folds):\n",
    "    fold = []\n",
    "    while len(fold) < fold_size:\n",
    "      fold.append(dataset_.pop(randrange(len(dataset_))))\n",
    "    dataset_folds.append(fold)\n",
    "\n",
    "  return dataset_folds\n",
    "\n",
    "\n",
    "def accuracy_metric(actual, predicted):\n",
    "  assert len(actual) == len(predicted)\n",
    "\n",
    "  correct_count = 0\n",
    "  size = len(actual)\n",
    "\n",
    "  for i in range(size):\n",
    "    correct_count += 1 if actual[i] == predicted[i] else 0\n",
    "  \n",
    "  return (correct_count / float(size)) * 100.0\n",
    "\n",
    "\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "  folds = cross_validation_split(dataset=dataset, n_folds=n_folds)\n",
    "  scores = []\n",
    "\n",
    "  for fold in folds:\n",
    "    train = folds.copy()\n",
    "    train.remove(fold)\n",
    "    train = sum(train, [])\n",
    "    test = []\n",
    "    for row in fold:\n",
    "      row_ = row.copy()\n",
    "      row_[-1] = None\n",
    "      test.append(row_)\n",
    "    predicted = algorithm(train, test, *args)\n",
    "    actual = list(map(lambda row: row[-1], fold))\n",
    "    accuracy = accuracy_metric(actual=actual, predicted=predicted)\n",
    "    scores.append(accuracy)\n",
    "  \n",
    "  return scores\n",
    "\n",
    "\n",
    "def euclidean_distance(row1, row2):\n",
    "  assert len(row1) == len(row2)\n",
    "\n",
    "  distance = 0.0\n",
    "\n",
    "  for i in range(len(row1) - 1):\n",
    "    distance += (row1[i] - row2[i]) ** 2\n",
    "\n",
    "  return sqrt(distance)\n",
    "\n",
    "\n",
    "def get_k_neighbors(dataset, row, k):\n",
    "  distances = []\n",
    "\n",
    "  for dataset_row in dataset:\n",
    "    distances.append((dataset_row, euclidean_distance(row, dataset_row)))\n",
    "  distances.sort(key=lambda tup: tup[1])\n",
    "\n",
    "  return [distances[i][0] for i in range(k)]\n",
    "\n",
    "\n",
    "def predict_classification(dataset, row, k):\n",
    "  neighbors = get_k_neighbors(dataset=dataset, row=row, k=k)\n",
    "  class_vals = list(map(lambda row: row[-1], neighbors))\n",
    "  \n",
    "  return max(set(class_vals), key=class_vals.count)\n",
    "\n",
    "\n",
    "def k_nearest_neighbors(train, test, k):\n",
    "  predictions = []\n",
    "\n",
    "  for row in test:\n",
    "    predictions.append(predict_classification(dataset=train, row=row, k=k))\n",
    "  \n",
    "  return predictions\n",
    "\n",
    "\n",
    "filepath = \"../datasets/abalone.csv\"\n",
    "dataset = load_csv(filepath=filepath)\n",
    "for col in range(1, len(dataset[0])):\n",
    "  str_col_to_float(dataset=dataset, col=col)\n",
    "str_col_to_int(dataset=dataset, col=0)\n",
    "n_folds = 5\n",
    "k_neighbors = 5\n",
    "scores = evaluate_algorithm(dataset, k_nearest_neighbors, n_folds, k_neighbors)\n",
    "display(f\"scores={scores}\")\n",
    "display(f\"mean accuracy={sum(scores) / len(scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d90add59a33e9f0fdc2a403872f22c3f38cf2bdffc6f1ab0ac261e13b4722f13"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
